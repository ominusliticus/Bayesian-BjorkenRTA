{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For changing directories to C++ programming and runnning files\n",
    "import subprocess as sp\n",
    "import os\n",
    "\n",
    "# since using WSL2\n",
    "os.environ['MPLCONFIGDIR'] = '/tmp/'\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# Typical functionality for data manipulation and generation of latin hypercube\n",
    "import numpy as np\n",
    "from pyDOE import lhs\n",
    "import emcee\n",
    "\n",
    "# Gaussian Process emulator \n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor as gpr\n",
    "from sklearn.gaussian_process import kernels as krnl\n",
    "import scipy.special as special\n",
    "import scipy.stats as st\n",
    "from scipy import optimize\n",
    "from scipy.linalg import lapack\n",
    "\n",
    "# for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as tck\n",
    "import seaborn as sns\n",
    "sns.set(\"notebook\")\n",
    "%matplotlib inline\n",
    "plt.style.use('default')\n",
    "\n",
    "# data storage\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linux\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.platform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My costumizations for plots\n",
    "import matplotlib.font_manager\n",
    "from matplotlib import rc\n",
    "rc('font',**{'family':'serif', 'serif':['Computer Modern Roman']})\n",
    "rc('text', usetex=True)\n",
    "\n",
    "def get_cmap(n, name='hsv'):\n",
    "    '''Returns a function that maps each index in 0, 1, ..., n-1 to a distinct \n",
    "    RGB color; the keyword argument name must be a standard mpl colormap name.'''\n",
    "    return plt.cm.get_cmap(name, n)\n",
    "\n",
    "def costumize_axis(ax, x_title, y_title):\n",
    "    ax.set_xlabel(x_title, fontsize=24)\n",
    "    ax.set_ylabel(y_title, fontsize=24)\n",
    "    ax.tick_params(axis='both', labelsize=18, top=True, right=True)\n",
    "    ax.tick_params(axis='both', which='major', direction='in', length=8)\n",
    "    ax.xaxis.set_minor_locator(tck.AutoMinorLocator())\n",
    "    ax.yaxis.set_minor_locator(tck.AutoMinorLocator())\n",
    "    ax.tick_params(axis='both', which='minor', direction='in', length=4, top=True, right=True)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Others costumizations for plots\n",
    "def hist_1d_2d(X, Y, nameX, nameY):\n",
    "    left, width = 0.1, 0.75\n",
    "    bottom, height = 0.1, 0.75\n",
    "    spacing = 0.005\n",
    "    rect_scatter = [left, bottom, width, height]\n",
    "    rect_histx = [left, bottom + height + spacing, width, 0.15]\n",
    "    rect_histy = [left + width + spacing, bottom, 0.15, height]\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    fig.patch.set_facecolor('white')\n",
    "    ax = fig.add_axes(rect_scatter)\n",
    "    ax1 = fig.add_axes(rect_histx, sharex=ax)\n",
    "    ax2 = fig.add_axes(rect_histy, sharey=ax)\n",
    "    costumize_axis(ax1, '', '')\n",
    "    costumize_axis(ax2, '', '')\n",
    "    ax1.tick_params(axis=\"x\", labelbottom=False)\n",
    "    ax2.tick_params(axis=\"y\", labelleft=False)\n",
    "\n",
    "    ax.scatter(X, Y, color='blue')\n",
    "    ax1.hist(X, color='blue', density=True)\n",
    "    ax2.hist(Y, color='blue', orientation='horizontal', density=True)\n",
    "    costumize_axis(ax, nameX, nameY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the Model\n",
    "This was actually done in `C++`, and this bit of code interface with the executable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PrintParametersFile(params_dict):\n",
    "    '''\n",
    "    Function ouputs file \"params.txt\" to the Code/util folder to be used by the\n",
    "    Code/build/exact_solution.x program\n",
    "    '''\n",
    "    os.chdir('/mnt/c/Users/gil-c/Documents/Heinz_Research/TeX-Docs/Rough-Drafts/Bayesian-Toy-Model/Code')\n",
    "    with open('./utils/params.txt', 'w') as fout:\n",
    "        fout.write(f'tau_0 {params_dict[\"tau_0\"]}\\n')\n",
    "        fout.write(f'Lambda_0 {params_dict[\"Lambda_0\"]}\\n')\n",
    "        fout.write(f'alpha_0 {params_dict[\"alpha_0\"]}\\n')\n",
    "        fout.write(f'xi_0 {params_dict[\"xi_0\"]}\\n')\n",
    "        fout.write(f'ul {params_dict[\"tau_f\"]}\\n')\n",
    "        fout.write(f'll {params_dict[\"tau_0\"]}\\n')\n",
    "        fout.write(f'mass {params_dict[\"mass\"]}\\n')\n",
    "        fout.write(f'eta_s {params_dict[\"eta_s\"]}\\n')\n",
    "        fout.write(f'pl0 {params_dict[\"pl0\"]}\\n')\n",
    "        fout.write(f'pt0 {params_dict[\"pt0\"]}\\n')\n",
    "        fout.write(f'TYPE {params_dict[\"hydro_type\"]}')\n",
    "    os.chdir('/mnt/c/Users/gil-c/Documents/Heinz_Research/TeX-Docs/Rough-Drafts/Bayesian-Toy-Model/Code/scripts/')\n",
    "    return \n",
    "\n",
    "def RunHydroSimulation():\n",
    "    '''\n",
    "    Function calls the C++ excecutable that run hydro calculations\n",
    "    '''\n",
    "    os.chdir('../')\n",
    "    sp.run(['./build/exact_solution.x'], shell=True)\n",
    "    os.chdir('scripts/')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default values for parameters\n",
    "params = {\n",
    "    'tau_0': 0.1,\n",
    "    'Lambda_0': 1.647204044,\n",
    "    'xi_0': -0.8320365099,\n",
    "    'alpha_0': 0.654868759,\n",
    "    'tau_f': 12.1,\n",
    "    'mass': 1.015228426,\n",
    "    'eta_s': 0.23873241463784,\n",
    "    'pl0': 8.1705525351457684,\n",
    "    'pt0': 1.9875332965147663,\n",
    "    'hydro_type': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function run runs hydro code to generate simualtion results for a set of \n",
    "# given parameters\n",
    "def ProcessHydro(parameter_names, simulation_points, store_whole_file=False):\n",
    "    out_list = []\n",
    "    def GetFromOutputFiles(hydro_type):\n",
    "        if hydro_type == 0:\n",
    "            prefix = '../output/CE_hydro/'\n",
    "            suffix = ''\n",
    "        elif hydro_type == 1:\n",
    "            prefix = '../output/DNMR_hydro/'\n",
    "            suffix = ''\n",
    "        elif hydro_type == 2:\n",
    "            prefix = '../output/aniso_hydro/'\n",
    "            suffix = ''\n",
    "        elif hydro_type == 3:\n",
    "            prefix = '../output/aniso_hydro/'\n",
    "            suffix = '2'\n",
    "        \n",
    "        if store_whole_file:\n",
    "            f_e = open(prefix + 'e' + suffix + '_m=0.200GeV.dat', 'r').readlines()\n",
    "            f_pi = open(prefix + 'shear' + suffix + '_m=0.200GeV.dat', 'r').readlines()\n",
    "            f_Pi = open(prefix + 'bulk' + suffix + '_m=0.200GeV.dat', 'r').readlines()\n",
    "            \n",
    "            out_list = []\n",
    "            for i in range(len(f_e)):\n",
    "                tau, e, pi, Pi, p = f_e[i].split()[0], f_e[i].split()[1], f_pi[i].split()[1], f_Pi[i].split()[1], f_e[i].split()[2]\n",
    "                out_list.append([float(tau), float(e), float(pi), float(Pi), float(p)])\n",
    "\n",
    "            return np.array(out_list)\n",
    "        else:\n",
    "            f_e = open(prefix + 'e' + suffix + '_m=0.200GeV.dat', 'r')\n",
    "            last_e = f_e.readlines()[-1]\n",
    "            tau, e = last_e.split()[0], last_e.split()[1]\n",
    "            f_e.close(); del last_e\n",
    "\n",
    "            f_pi = open(prefix + 'shear' + suffix + '_m=0.200GeV.dat', 'r')\n",
    "            last_pi = f_pi.readlines()[-1]\n",
    "            pi = last_pi.split()[1]\n",
    "            f_pi.close(); del last_pi\n",
    "\n",
    "            f_Pi = open(prefix + 'bulk' + suffix + '_m=0.200GeV.dat', 'r')\n",
    "            last_Pi = f_Pi.readlines()[-1]\n",
    "            Pi = last_Pi.split()[1]\n",
    "            f_Pi.close(); del last_Pi\n",
    "\n",
    "            temp_list = [float(tau), float(e), float(pi), float(Pi)]\n",
    "            return np.array(temp_list)\n",
    "\n",
    "    def GetExactResults():\n",
    "        f_exact = open('../output/exact/MCMC_calculation_moments.dat','r')\n",
    "        if store_whole_file:\n",
    "            return f_exact.readlines()\n",
    "        else:\n",
    "            t, e, pl, pt, p = f_exact.readlines()[-1].split()\n",
    "            pi = (float(pt) - float(pl)) / 1.5\n",
    "            Pi = (2 *  float(pt) + float(pl)) / 3 - float(p)\n",
    "            temp_list = [float(t), float(e), pi, Pi]\n",
    "            return temp_list\n",
    "\n",
    "    if len(simulation_points) > len(parameter_names):\n",
    "        for parameters in simulation_points:\n",
    "            for i, name in enumerate(parameter_names):\n",
    "                params[name] = parameters[i]\n",
    "            PrintParametersFile(params)\n",
    "            RunHydroSimulation()\n",
    "            if params['hydro_type'] == 4:\n",
    "                out_list.append(GetExactResults())\n",
    "            else:\n",
    "                out_list.append(GetFromOutputFiles(params['hydro_type']))\n",
    "\n",
    "    else:\n",
    "        for i, name in enumerate(parameter_names):\n",
    "            params[name] = simulation_points[i]\n",
    "        PrintParametersFile(params)\n",
    "        RunHydroSimulation()\n",
    "        if params['hydro_type'] == 4:\n",
    "            return np.array(GetExactResults())\n",
    "        else:\n",
    "            return np.array(GetFromOutputFiles(params['hydro_type']))\n",
    "\n",
    "    return np.array(out_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Process Regression\n",
    "The training points and validation plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulation times for generation of psuedo data\n",
    "simulation_taus = np.array([5.1, 6.1, 7.1, 8.1, 9.1, 10.1, 11.1, 12.1])\n",
    "eta_s = 5.0 / (4 * np.pi)\n",
    "params['eta_s'] = eta_s\n",
    "params['hydro_type'] = 4;\n",
    "exact_out = []\n",
    "\n",
    "read_in_exact = True\n",
    "if read_in_exact:\n",
    "    with open('hydro_simulation_points/exact_output_various_times.dat','r') as f:\n",
    "        exact_out = np.array([[float(entry) for entry in line.split()] for line in f.readlines()])\n",
    "else:\n",
    "\n",
    "    # Need to improve to run calculation once, and then back track to the points we want to keep\n",
    "    for i, tau in enumerate(simulation_taus):\n",
    "        exact_out.append(ProcessHydro(['tau_f'], [tau]))\n",
    "    \n",
    "    with open('hydro_simulation_points/exact_output_various_times.dat','w') as f:\n",
    "        for i, tau in enumerate(simulation_taus):\n",
    "            for entry in exact_out[i]:\n",
    "                f.write(f'{entry} ')\n",
    "            if i != simulation_taus.size - 1:\n",
    "                f.write('\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.71619724]\n"
     ]
    }
   ],
   "source": [
    "# Need to define latin hypercube sampling points and then run hydrodynamic simulation for all.\n",
    "num_parameters_in_GP = 1 # Should be minimum of two to take into account relaxation time constant and ending time\n",
    "GP_parameter_names = ['eta_s'] # Expand as needed\n",
    "GP_parameter_names_math = [r'$\\eta_s$']\n",
    "# Make sure that order of ranges variable matches how they appear in names_of_GP_parameters\n",
    "GP_parameter_ranges = np.array([[1 / (4 * np.pi), 10 / (4 * np.pi)]])\n",
    "num_design_points = 40 * num_parameters_in_GP\n",
    "unit = lhs(n=num_parameters_in_GP, samples=num_design_points, criterion='maximin')\n",
    "GP_design_points = GP_parameter_ranges[:, 0] + unit * (GP_parameter_ranges[:, 1] - GP_parameter_ranges[:, 0])\n",
    "GP_design_range = GP_parameter_ranges[:,1] - GP_parameter_ranges[:,0]\n",
    "print(GP_design_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def design_log_prior(model_parameters, design_range):\n",
    "    '''\n",
    "    Parameters:\n",
    "    ------------\n",
    "    model_parameters    - 1d-array with shape (n,). Value of parameters used to evaluate model\n",
    "    design_range        - 2d-array with shape (n,2). Give upper and lower limits of parameter values\n",
    "    '''\n",
    "    X  = np.array(model_parameters).reshape(1,-1)\n",
    "    lower = np.all(X >= np.array(design_range)[:,0])\n",
    "    upper = np.all(X <= np.array(design_range)[:,1])\n",
    "\n",
    "    if (lower and upper):\n",
    "        return 0\n",
    "    else:\n",
    "        return -np.inf\n",
    "\n",
    "def design_log_likelihood(y, cov):\n",
    "    '''\n",
    "    Parameters:\n",
    "    ------------\n",
    "    y   - 1d-array with shape (n,)\n",
    "    cov - 2d-array with shape (n,n)\n",
    "    '''\n",
    "    # Use Cholesky decomposition for efficient lin alg algo\n",
    "    L, info = lapack.dpotrf(cov, clean=True)\n",
    "\n",
    "    if (info < 0):\n",
    "        raise print('Error occured in computation of Cholesky decomposition')\n",
    "\n",
    "    # Solve equation L*b=y\n",
    "    b, info = lapack.dpotrs(L, np.array(y))\n",
    "\n",
    "    if (info != 0):\n",
    "        raise print('Error in inverting matrix equation')\n",
    "\n",
    "    if np.all(L.diagonal() > 0):\n",
    "        return -0.5 * np.dot(y, b) - np.log(L.diagonal()).sum()\n",
    "    else:\n",
    "        raise print('Diagonal has negative entry')\n",
    "\n",
    "def design_log_posterior(model_parameters, design_range, design_points, design_points_error, truth, truth_error):\n",
    "    '''\n",
    "    Parameters:\n",
    "    ------------\n",
    "    model_parameters    - 1d-array like (n,)\n",
    "    design_range        - 2d-array like (n,2)\n",
    "    desing_points       - 1d-array like (3,), contains observables\n",
    "    truth               - 1d-array like (3,), psuedo experimental data\n",
    "    truth_error         - 1d-array like (3,), fake error-bar on psuedo-data\n",
    "    '''\n",
    "    y = design_points - truth\n",
    "    # cov = 1e-5 * np.eye(3)\n",
    "    cov = np.diag(design_points_error) ** 2\n",
    "    cov += np.diag(truth_error) ** 2\n",
    "\n",
    "    return design_log_prior(model_parameters, design_range) + design_log_likelihood(y, cov)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ce = []    # 0\n",
    "# dnmr = []  # 1\n",
    "# vah = []   # 2\n",
    "# mvah = []  # 3\n",
    "hydro_mode = [0, 1, 2, 3]\n",
    "names = ['ce', 'dnmr', 'vah', 'mvah']\n",
    "read_in_simulation = False   # should false for first run\n",
    "if read_in_simulation:\n",
    "    with open('design_points/design_points_n=1.dat','r') as f:\n",
    "        GP_design_points = np.array([[float(entry) for entry in line.split()] for line in f.readlines()])\n",
    "\n",
    "    hydro_simulations = dict((key, []) for key in names)\n",
    "\n",
    "    for j, tau in enumerate(simulation_taus):\n",
    "        for k, name in enumerate(names):\n",
    "            with open(f'hydro_simulation_points/{name}_simulation_points_n=1_tau={tau}.dat', 'r') as f:\n",
    "                lines = f.readlines()\n",
    "                hydro_simulations[name].append(np.array([[float(entry) for entry in line.split()] for line in lines]))\n",
    "\n",
    "    full_simulations = dict((key, []) for key in names)\n",
    "    for k, name in enumerate(names):\n",
    "        for i, design_point in enumerate(GP_design_points):\n",
    "            with open(f'full_outputs/{name}_full_output_C={design_point}.dat', 'r') as f:\n",
    "                for line in f.readlines():\n",
    "                    temp = []\n",
    "                    for entry in line.split():\n",
    "                        temp.append(float(entry))\n",
    "                    full_simulations[name].append(temp)\n",
    "else:\n",
    "    global_last_output = dict((key, []) for key in names)\n",
    "    global_full_output = dict((key, []) for key in names)\n",
    "\n",
    "    tau_start = params['tau_0']\n",
    "    delta_tau = tau_start / 20\n",
    "    n_steps_1_fm = 1 / delta_tau\n",
    "    params['tau_f'] = simulation_taus[-1]\n",
    "\n",
    "    for i in range(4):\n",
    "        params['hydro_type'] = i\n",
    "        for design_point in GP_design_points:\n",
    "            local_last_output = []\n",
    "            hydro_output = ProcessHydro(parameter_names=GP_parameter_names, simulation_points=design_point, store_whole_file=True)\n",
    "            for j in np.arange(int(simulation_taus[0]), int(simulation_taus[-1])+1, 1):\n",
    "                local_last_output.append(hydro_output[int(j * n_steps_1_fm) - 1, :])\n",
    "            global_last_output[names[i]].append(local_last_output)\n",
    "            global_full_output[names[i]].append(hydro_output)   \n",
    "\n",
    "    # print(os.getcwd())\n",
    "    # with open('design_points/design_points_n=1.dat','w') as f:\n",
    "    #     for line in GP_design_points:\n",
    "    #         for entry in line:\n",
    "    #             f.write(f'{entry} ')\n",
    "    #         f.write(f'\\n')\n",
    "\n",
    "    # for k, name in enumerate(names):\n",
    "    #     for j, tau in enumerate(simulation_taus):\n",
    "    #             with open(f'hydro_simulation_points/{name}_simulation_points_n=1_tau={tau}.dat', 'w') as f:\n",
    "    #                 for line in np.array(global_last_output[name])[:, j, :]:\n",
    "    #                     for entry in line:\n",
    "    #                         f.write(f'{entry} ')\n",
    "    #                     f.write('\\n')\n",
    "\n",
    "    # for k, name in enumerate(names):\n",
    "    #     for i, design_point in enumerate(GP_design_points):\n",
    "    #         with open(f'full_outputs/{name}_full_output_C={design_point}.dat', 'w') as f:\n",
    "    #             for line in np.array(global_full_output[name])[i]:\n",
    "    #                 for entry in line:\n",
    "    #                     f.write(f'{entry} ')\n",
    "    #                 f.write('\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4., 4., 4., 4., 4., 4., 3., 5., 4., 4.]),\n",
       " array([0.08461014, 0.15525808, 0.22590602, 0.29655396, 0.3672019 ,\n",
       "        0.43784984, 0.50849778, 0.57914572, 0.64979366, 0.7204416 ,\n",
       "        0.79108954]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# fig, ax = plt.subplots(figsize=(10, 10))\n",
    "# fig.patch.set_facecolor('white')\n",
    "# ax.scatter(GP_design_points[:, 0], GP_design_points[:, 1])\n",
    "# costumize_axis(ax, r'$\\eta/\\mathcal S$', r'$\\tau_f$ [fm/c]')\n",
    "\n",
    "#hist_1d_2d(GP_design_points[:, 0], GP_design_points[:, 1], r'$\\eta/\\mathcal S$', r'$\\tau_f$ [fm/c]')\n",
    "plt.hist(GP_design_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 40, 5)\n",
      "8\n",
      "(8, 40, 5)\n",
      "True\n",
      "[[0.71619724]]\n"
     ]
    }
   ],
   "source": [
    "arr = np.array([np.array(global_last_output['ce'])[:, j, :] for j in range(simulation_taus.size)])\n",
    "print(arr.shape)\n",
    "print(simulation_taus.size)\n",
    "print(np.array(hydro_simulations['ce']).shape)\n",
    "print(np.all(arr == np.array(hydro_simulations['ce'])))\n",
    "print(np.diff(GP_parameter_ranges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make plots of total output\n",
    "fig, ax = plt.subplots(1, 3, figsize=(30,10))\n",
    "fig.patch.set_facecolor('white')\n",
    "cmap = get_cmap(10, 'tab10')\n",
    "\n",
    "delta_tau = params['tau_0'] / 20\n",
    "steps = int((params['tau_f'] - params['tau_0']) / delta_tau)\n",
    "rand_int = np.random.randint(0, num_design_points)\n",
    "for i in range(3):\n",
    "    for j, name in enumerate(names):\n",
    "        data = np.array(full_simulations[name])\n",
    "        for k in range(num_design_points - 1):\n",
    "            # fancy logic since all output is stored in one go\n",
    "            if i == 0:\n",
    "                y = data[k * steps: (k + 1) * steps - 1, i+1] / data[k * steps, i+1]\n",
    "            else:\n",
    "                y = data[k * steps: (k + 1) * steps - 1, i+1] / (data[k * steps: (k + 1) * steps - 1, 1]  + data[k * steps: (k + 1) * steps - 1, 3])\n",
    "\n",
    "            if k == rand_int: \n",
    "                ax[i].plot(data[k * steps : (k + 1) * steps - 1, 0], y, lw=2, color=cmap(j), label=name)\n",
    "            else:\n",
    "                ax[i].plot(data[k * steps : (k + 1) * steps - 1, 0], y, color=cmap(j), alpha=0.1)\n",
    "costumize_axis(ax[0], r'$\\tau$ [fm/c]', r'$\\mathcal E /\\mathcal E_0$')\n",
    "costumize_axis(ax[1], r'$\\tau$ [fm/c]', r'$\\pi / (\\mathcal E + \\mathcal P_\\mathrm{eq})$')\n",
    "costumize_axis(ax[2], r'$\\tau$ [fm/c]', r'$\\Pi / (\\mathcal E + \\mathcal P_\\mathrm{eq})$')\n",
    "ax[0].set_yscale('log')\n",
    "ax[0].legend(loc='upper right', fontsize=25)\n",
    "\n",
    "fig.tight_layout()\n",
    "# fig.savefig('plots/full_simulations.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4*np.pi*GP_design_points[rand_int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hydro_lists = np.array([hydro_simulations[key] for key in hydro_simulations])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.10000000e+00, 1.12457544e-01, 1.51927051e-02, 4.19141696e-04],\n",
       "       [6.10000000e+00, 8.23325425e-02, 1.18333583e-02, 2.41511364e-04],\n",
       "       [7.10000000e+00, 7.96084902e-02, 7.94512551e-03, 1.76996524e-04],\n",
       "       [8.10000000e+00, 6.98990144e-02, 6.25718945e-03, 1.11830499e-04],\n",
       "       [9.10000000e+00, 5.30328132e-02, 5.08986212e-03, 7.63120013e-05],\n",
       "       [1.01000000e+01, 4.82324295e-02, 4.12790520e-03, 4.12022155e-05],\n",
       "       [1.11000000e+01, 4.61667682e-02, 3.57687052e-03, 2.44168317e-05],\n",
       "       [1.21000000e+01, 4.05668156e-02, 3.19181520e-03, 9.96514027e-06]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha_error = 1.0\n",
    "exact_error = alpha_error * exact_out[:, 1:4]\n",
    "exact_pseudo = np.array([[np.random.normal(loc=obv, scale=0.05 * obv) for obv in exact] for exact in exact_out])\n",
    "for i, tau in enumerate(simulation_taus):\n",
    "    exact_pseudo[i, 0] = tau\n",
    "exact_pseudo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make posteriors for GP_design points, given eta_s = 5. / (4 pi) and tau_f = 15\n",
    "names = ['ce', 'dnmr', 'vah', 'mvah']\n",
    "posteriors = {}\n",
    "sort_arr = np.argsort(GP_design_points, 0)\n",
    "for i, name in enumerate(names):\n",
    "    final_arr = np.zeros(num_design_points)\n",
    "    for j in range(num_design_points):\n",
    "        post = 0\n",
    "        for k, tau in enumerate(simulation_taus):\n",
    "            design_point = GP_design_points[j]\n",
    "            hydro_observables = hydro_lists[i, k, j, 1:4]\n",
    "            hydro_observables_error = 0.0 * hydro_observables\n",
    "            post += design_log_posterior(design_point, GP_parameter_ranges, hydro_observables, hydro_observables_error, exact_pseudo[k, 1:4], exact_error[k])\n",
    "        final_arr[j] = post\n",
    "    eta_s_posterior = final_arr[sort_arr[:,0]]\n",
    "    posteriors[name] = np.array(eta_s_posterior)\n",
    "fig, ax = plt.subplots(1, 2, figsize=(20, 10))\n",
    "for i, name in enumerate(names):\n",
    "    # include log-posterior plot\n",
    "    # normalize data \n",
    "    print(np.max(np.exp(posteriors[name])), np.max(posteriors[name]))\n",
    "    if True: #name != 'vah':\n",
    "        ax[0].plot(GP_design_points[sort_arr[:,0]], np.exp(posteriors[name]) / np.sum(np.exp(posteriors[name])), label=name)\n",
    "        ax[1].plot(GP_design_points[sort_arr[:,0]], posteriors[name] - np.max(posteriors[name]))\n",
    "ax[0].legend(loc='upper right', fontsize=25)\n",
    "costumize_axis(ax[0], r'$\\mathcal C$', 'normalized posterior')\n",
    "costumize_axis(ax[1], r'$\\mathcal C$', 'log-posterior')\n",
    "fig.tight_layout()\n",
    "#fig.savefig(\"plots/training-data-posterior_n=1.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.exp(posteriors['vah']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "if False:\n",
    "    error_alpha = [100, 10, 1, 0.1, 0.01]\n",
    "    for k, err in enumerate(error_alpha):\n",
    "        # make posteriors for GP_design points, given eta_s = 5. / (4 pi) and tau_f = 15\n",
    "        names = ['ce', 'dnmr', 'vah', 'mvah']\n",
    "        posteriors = {}\n",
    "        sort_arr = np.argsort(GP_design_points, 0)\n",
    "        for i, name in enumerate(names):\n",
    "            final_arr = np.zeros(num_design_points)\n",
    "            for j in range(num_design_points):\n",
    "                post = 0\n",
    "                design_point = GP_design_points[j]\n",
    "                for m, tau in enumerate(simulation_taus):\n",
    "                    hydro_observables = hydro_lists[i, m, j, 1:4]\n",
    "                    hydro_observables_error = 0.0 * hydro_observables\n",
    "                    post += design_log_posterior(design_point, GP_parameter_ranges, hydro_observables, hydro_observables_error, exact_pseudo[m, 1:4], err * exact_out[m, 1:4])\n",
    "                    final_arr[j] = post\n",
    "            eta_s_posterior = final_arr[sort_arr[:,0]]\n",
    "            posteriors[name] = np.array(eta_s_posterior)\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "        for i, name in enumerate(names):\n",
    "            ax.plot(GP_design_points[sort_arr[:,0]], np.exp(posteriors[name] - np.max(posteriors[name])), label=name)\n",
    "        ax.plot([0.1], [-1], color='white', label=f'error bar = {100 * err}%')\n",
    "        ax.legend(loc='upper right', fontsize=25)\n",
    "        costumize_axis(ax, r'$\\mathcal C$', '')\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(f'plots/design_points_posterior-error={err}.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%capture\n",
    "scalers = dict((key, []) for key in names)\n",
    "emulators = dict((key, []) for key in names)\n",
    "\n",
    "read_in_emulator = True\n",
    "if read_in_emulator:\n",
    "    pickle_scalers = open('pickle_files/scalers_data.pkl','rb')\n",
    "    pickle_emulators = open('pickle_files/emulators_data.pkl', 'rb')\n",
    "    scalers = pickle.load(pickle_scalers)\n",
    "    emulators = pickle.load(pickle_emulators)\n",
    "    pickle_scalers.close()\n",
    "    pickle_emulators.close()\n",
    "else:\n",
    "    f = open('emulator_scores.txt', 'w')\n",
    "    pickle_scalers = open('pickle_files/scalers_data.pkl', 'wb')\n",
    "    pickle_emulators = open('pickle_files/emulators_data.pkl', 'wb')\n",
    "    for i, name in enumerate(names):\n",
    "        global_scalers = []\n",
    "        global_emulators = []\n",
    "        for j, tau in enumerate(simulation_taus):\n",
    "            local_scalers = []\n",
    "            local_emulators = []\n",
    "            f.write(f'\\tTraining GP for {name}\\n')\n",
    "            for m in range(1, 4):\n",
    "                bounds = np.outer(GP_design_range[0], (1e-2, 100))\n",
    "                # StandardScaler takes mean and std dev of every column and calculates z-score\n",
    "                SS = StandardScaler().fit(hydro_lists[i][j,:,m].reshape(-1,1))\n",
    "                local_scalers.append(SS)\n",
    "                data = SS.transform(hydro_lists[i][j,:,m].reshape(-1,1))\n",
    "\n",
    "                kernel = 1 * krnl.RBF(length_scale=GP_design_range, length_scale_bounds=bounds) \n",
    "                GPR = gpr(kernel=kernel, n_restarts_optimizer=30)\n",
    "                f.write(f'\\t\\tTraining GP for {name} and time {tau}\\n')\n",
    "                GPR.fit(GP_design_points, data)\n",
    "                f.write('GP score: {:1.3f}\\n'.format(GPR.score(GP_design_points, data)))\n",
    "                local_emulators.append(GPR)\n",
    "            global_scalers.append(local_scalers)\n",
    "            global_emulators.append(local_emulators)\n",
    "        scalers[name] = global_scalers\n",
    "        emulators[name] = global_emulators\n",
    "    pickle.dump(scalers, pickle_scalers)\n",
    "    pickle.dump(emulators, pickle_emulators)\n",
    "\n",
    "    f.close()\n",
    "    pickle_emulators.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    print(emulators['ce'][0][i].kernel_.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler().fit(hydro_lists[0][0,:,1].reshape(-1,1))\n",
    "ss.scale_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(30,10))\n",
    "fig.patch.set_facecolor('white')\n",
    "\n",
    "names = ['ce', 'dnmr','vah','mvah']\n",
    "observables = [r'$\\mathcal E$ [fm$^{-1}$]', r'$\\pi$ [fm$^{-1}$]', r'$\\Pi$ [fm$^{-1}$]']\n",
    "cmap = get_cmap(10, 'tab10')\n",
    "x_vals = np.linspace(GP_parameter_ranges[:,0], GP_parameter_ranges[:,1], 1000)\n",
    "for i, name in enumerate(names):\n",
    "    for j, observable in enumerate(observables):\n",
    "        for k, tau in enumerate(simulation_taus):\n",
    "            if k == 0:\n",
    "                ax[j].scatter(GP_design_points[sort_arr[:,0]], hydro_lists[i,k,sort_arr[:,0],j+1], lw=2, color=cmap(i), marker='o', label=name)\n",
    "            else:\n",
    "                ax[j].scatter(GP_design_points[sort_arr[:,0]], hydro_lists[i,k,sort_arr[:,0],j+1], lw=2, color=cmap(i), marker='o')\n",
    "\n",
    "            # deal with inverse transfomring errors: boot-strapped method\n",
    "            emul_prime, err_prime = emulators[name][k][j].predict(x_vals.reshape(-1,1), return_std=True)\n",
    "            err_prime = err_prime.reshape(-1,1)\n",
    "            emul = scalers[name][k][j].inverse_transform(emul_prime)\n",
    "            err_plus = scalers[name][k][j].inverse_transform(emul_prime + err_prime) - emul\n",
    "            err_minus = emul - scalers[name][k][j].inverse_transform(emul_prime - err_prime) \n",
    "            err = np.sqrt(err_plus ** 2 + err_minus ** 2).reshape(-1,)\n",
    "\n",
    "            # plot inverserved transformed predictions\n",
    "            emul = emul.reshape(-1,)\n",
    "            err = err.reshape(-1,)\n",
    "            ax[j].fill_between(x_vals.reshape(-1,), emul + err, emul - err, color=cmap(i), alpha=0.5)\n",
    "            #ax[j].plot(x_vals, emul, color=cmap(i))\n",
    "            costumize_axis(ax[j], r'$\\mathcal C$', observable)\n",
    "ax[1].legend(loc='upper left', fontsize=25)\n",
    "fig.tight_layout()\n",
    "#fig.savefig(\"plots/emulator-performance_n=1.pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#validation\n",
    "if False:\n",
    "    number_validation_points = 10\n",
    "    tau_f = np.full((number_validation_points, 1), 15)\n",
    "    eta_s = np.array(GP_parameter_ranges[0,0] + np.random.rand(number_validation_points, 1) * GP_design_range[0])\n",
    "    eta_s = eta_s[np.argsort(eta_s, 0)[:,0]]\n",
    "    validation_points = eta_s.reshape(-1, 1)\n",
    "\n",
    "    names = ['ce', 'dnmr', 'vah', 'mvah']\n",
    "\n",
    "    validation_data = {}\n",
    "    for i, name in enumerate(names):\n",
    "        params['hydro_type'] = i\n",
    "        validation_data[name] = ProcessHydro(GP_parameter_names, validation_points)\n",
    "\n",
    "    GP_predictions = {'ce': [], 'dnmr': [], 'vah': [], 'mvah': []}\n",
    "    for name in names:\n",
    "        for i in range(3):\n",
    "            GP_x_prediction, GP_x_err = emulators[name][i].predict(validation_points, return_std=True)\n",
    "            GP_predictions[name].append(np.hstack((GP_x_prediction, GP_x_err.reshape(-1,1))))\n",
    "\n",
    "    # validation plot 1\n",
    "    fig, ax = plt.subplots(4, 3, figsize=(30,40))\n",
    "    fig.patch.set_facecolor('white')\n",
    "\n",
    "    names = ['ce', 'dnmr','vah','mvah']\n",
    "    observables = [r'$\\mathcal E$ [fm$^{-1}$]', r'$\\pi$ [fm$^{-1}$]', r'$\\Pi$ [fm$^{-1}$]']\n",
    "    cmap = get_cmap(10, 'tab10')\n",
    "    for i, name in enumerate(names):\n",
    "        for j, observable in enumerate(observables):\n",
    "            for k, tau in enumerate(simulation_taus):\n",
    "                # GP data\n",
    "                # ax[i,j].scatter(GP_design_points[sort_arr[:,0]], hydro_lists[i,sort_arr[:,0],j+1], lw=2, color='gray', marker='o', label=name)\n",
    "                emul, err = emulators[name][k][j].predict(GP_design_points, return_std=True)\n",
    "                emul = emul.reshape(-1,)[sort_arr[:,0]]\n",
    "                err = err[sort_arr[:,0]]\n",
    "                ax[i, j].fill_between(GP_design_points.reshape(-1,)[sort_arr[:,0]], emul + err, emul - err, color='black', alpha=0.4)\n",
    "                # Validation data\n",
    "                ax[i,j].scatter(validation_points[:,0], np.array(validation_data[name])[:, j+1], lw=2, color=cmap(i), label=name)\n",
    "                # ax[i,j].fill_between(validation_points[:,0], GP_predictions[name][j][:, 0] + GP_predictions[name][j][:, 1], GP_predictions[name][j][:, 0] - GP_predictions[name][j][:, 1], color=cmap(i), alpha=0.5)\n",
    "                costumize_axis(ax[i,j], r'$\\eta_\\mathcal S$ ', observable)\n",
    "    ax[0, 0].legend(loc=0, fontsize=25)\n",
    "    \n",
    "    # valaidation plot 2\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(30,10))\n",
    "    fig.patch.set_facecolor('white')\n",
    "\n",
    "    names = ['ce', 'dnmr','vah','mvah']\n",
    "    observables = [r'$\\mathcal E$ [fm$^{-1}$]', r'$\\pi$ [fm$^{-1}$]', r'$\\Pi$ [fm$^{-1}$]']\n",
    "    cmap = get_cmap(10, 'tab10')\n",
    "    for i, name in enumerate(names):\n",
    "        for j, observable in enumerate(observables):\n",
    "            ax[j].scatter(validation_points[:,0], np.array(validation_data[name])[:, j+1], lw=2, color=cmap(i), label=name)\n",
    "            ax[j].fill_between(validation_points[:,0], GP_predictions[name][j][:, 0] + GP_predictions[name][j][:, 1], GP_predictions[name][j][:, 0] - GP_predictions[name][j][:, 1], color=cmap(i), alpha=0.5)\n",
    "            costumize_axis(ax[j], r'$\\eta_\\mathcal S$ ', observable)\n",
    "    ax[0].legend(loc=0, fontsize=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check error distribution to make sure it conforms to out assumptions\n",
    "number_validation_points = 12\n",
    "eta_s = np.array(GP_parameter_ranges[0,0] + np.random.rand(number_validation_points, 1) * GP_design_range[0])\n",
    "eta_s = eta_s[np.argsort(eta_s, 0)[:,0]]\n",
    "validation_points = eta_s\n",
    "\n",
    "validate = False\n",
    "read_in_validation_data = True\n",
    "if validate:\n",
    "    if read_in_validation_data:\n",
    "        validation_data_file = open('pickle_files/validation_data.pkl', 'rb')\n",
    "        validation_data = pickle.load(validation_data_file)\n",
    "        validation_data_file.close()\n",
    "    else:\n",
    "        validation_data = dict((key, []) for key in names)\n",
    "        for i, name in enumerate(names):\n",
    "            params['hydro_type'] = i\n",
    "            local_list = []\n",
    "            for j, tau in enumerate(simulation_taus):\n",
    "                params['tau_f'] = tau\n",
    "                local_list.append(ProcessHydro(GP_parameter_names, validation_points))\n",
    "            validation_data[name].append(local_list)\n",
    "        \n",
    "        validation_data_file = open('pickle_files/validation_data.pkl', 'wb')\n",
    "        pickle.dump(validation_data, validation_data_file)\n",
    "        validation_data_file.close()\n",
    "\n",
    "    # This needs to be fixed: save the predictions to an output file as well\n",
    "    GP_predictions = dict((key, []) for key in names)\n",
    "    for name in names:\n",
    "        for i in range(3):\n",
    "            local_list = []\n",
    "            for j, tau in enumerate(simulation_taus):\n",
    "                GP_x_prediction, GP_x_err = emulators[name][j][i].predict(validation_points, return_std=True)\n",
    "                local_list.append(np.hstack((GP_x_prediction, GP_x_err.reshape(-1,1))))\n",
    "            GP_predictions[name].append(local_list)\n",
    "\n",
    "    normalized_discrepenancy = dict((key, []) for key in names)\n",
    "    for i, name in enumerate(names):\n",
    "        for j in range(3):\n",
    "            local_list = []\n",
    "            for k in range(simulation_taus.size):\n",
    "                local_list.append((np.array(GP_predictions[name])[j, k, :,0] - np.array(validation_data[name])[0, k, :, j+1]) / np.array(GP_predictions[name])[j, k, :,1])\n",
    "            normalized_discrepenancy[name].append(np.array(local_list).reshape(-1,1))\n",
    "\n",
    "    residuals = dict((key, []) for key in names)\n",
    "    for i, name in enumerate(names):\n",
    "        for j in range(3):\n",
    "            local_list = []\n",
    "            for k in range(simulation_taus.size):\n",
    "                local_list.append((np.array(GP_predictions[name])[j, k, :,0] - np.array(validation_data[name])[0, k, :, j+1]))\n",
    "            residuals[name].append(np.array(local_list).reshape(-1,1))\n",
    "\n",
    "    fig1, ax1 = plt.subplots(4, 3, figsize=(40, 30))\n",
    "    fig1.patch.set_facecolor('white')\n",
    "    for i, name in enumerate(names):\n",
    "        for j, observable in enumerate(observables):\n",
    "            ax1[i, j].hist(residuals[name][j], color='brown')\n",
    "            costumize_axis(ax1[i, j], observable, '')\n",
    "\n",
    "    fig2, ax2 = plt.subplots(4, 3, figsize=(40, 30))\n",
    "    fig2.patch.set_facecolor('white')\n",
    "    for i, name in enumerate(names):\n",
    "        for j, observable in enumerate(observables):\n",
    "            ax2[i, j].hist(normalized_discrepenancy[name][j], bins=15, color='brown')\n",
    "            costumize_axis(ax2[i, j], observable, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(validation_data['ce']).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_observable(model_parameters, hydro_name, tau_index, GP_emulator, scalers):\n",
    "    '''\n",
    "    Parameters:\n",
    "    ------------\n",
    "    model_parameters    - 1d-array like (n,) \n",
    "    hydro_name          - string\n",
    "    GP_emulator         - dictionary\n",
    "    '''\n",
    "    means = []\n",
    "    variances = []\n",
    "    for i in range(3):\n",
    "        prediction, error = GP_emulator[hydro_name][tau_index][i].predict(np.array(model_parameters).reshape(-1, len(model_parameters)), return_std=True)\n",
    "\n",
    "        # inverse transform error bars\n",
    "        scaler = scalers[hydro_name][tau_index][i]\n",
    "        error = error.reshape(-1,1)\n",
    "        mean = scaler.inverse_transform(prediction) \n",
    "        std_p = scaler.inverse_transform(prediction + error) - mean\n",
    "        std_m = mean - scaler.inverse_transform(prediction - error)\n",
    "        std = np.sqrt(std_p ** 2 + std_m ** 2).reshape(-1,)\n",
    "        mean = mean.reshape(-1,)\n",
    "\n",
    "        means.append(mean)\n",
    "        variances.append(std ** 2)\n",
    "    return np.hstack(means), np.diag(np.array(variances).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = predict_observable([0.33], 'ce', emulators)\n",
    "print(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Inference\n",
    "The usual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetTemperature(e, m):\n",
    "    def Eeq(T, z):\n",
    "        if z == 0:\n",
    "            return 3.0 * T ** 4 / np.pi ** 2\n",
    "        else:\n",
    "            return (3.0 * T ** 4 / np.pi ** 2) * (z ** 2 * special.kn(2, z) / 2.0 + z ** 3 * special.kn(1, z) / 6.0)\n",
    "\n",
    "    Tmin = 0.001 / 0.197\n",
    "    Tmax = 2.0 / 0.0197\n",
    "    n = 0; flag = True\n",
    "    Tmid = 0\n",
    "    while flag and n <= 2000:\n",
    "        Tmid = (Tmin + Tmax) / 2.0\n",
    "        emid = Eeq(Tmid, m / Tmid)\n",
    "        emin = Eeq(Tmin, m / Tmin)\n",
    "        if np.fabs(emid - e) < 1e-6:\n",
    "            break\n",
    "        if (emid - e) * (emin - e) <= 0:\n",
    "            Tmax = Tmid\n",
    "        else:\n",
    "            Tmin = Tmid\n",
    "        n += 1\n",
    "        if n == 1:\n",
    "            Tcopy = Tmid\n",
    "        if n > 4:\n",
    "            if np.fabs(Tcopy - Tmid) < 1e-6:\n",
    "                flag = False\n",
    "            Tcopy = Tmid\n",
    "\n",
    "    return Tmid * 0.197"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GetTemperature(12.4395115821926900, 0.2 / 0.197)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_parameters = [5 / (4 * np.pi)]\n",
    "params['hydro_type'] = 4\n",
    "true_observables = ProcessHydro(GP_parameter_names, true_parameters)\n",
    "true_temperature = GetTemperature(true_observables[1], params['mass']) / 0.197\n",
    "\n",
    "#true_observable_errors = [np.sqrt(true_observables[1]), np.sqrt(true_temperature ** 4 * np.fabs(true_observables[2])),np.sqrt(true_temperature ** 4 * np.fabs(true_observables[3]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_observable_errors = 0.0005 * np.fabs(true_observables[1:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(true_parameters)\n",
    "print(true_observables)\n",
    "print(true_observable_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_prior(model_parameters, design_range):\n",
    "    '''\n",
    "    Parameters:\n",
    "    ------------\n",
    "    model_parameters    - 1d-array with shape (n,). Value of parameters used to evaluate model\n",
    "    design_range        - 2d-array with shape (n,2). Give upper and lower limits of parameter values\n",
    "    '''\n",
    "    X  = np.array(model_parameters).reshape(1,-1)\n",
    "    lower = np.all(X >= np.array(design_range)[:,0])\n",
    "    upper = np.all(X <= np.array(design_range)[:,1])\n",
    "\n",
    "    if (lower and upper):\n",
    "        return 0\n",
    "    else:\n",
    "        return -np.inf\n",
    "\n",
    "def log_likelihood(y, cov):\n",
    "    '''\n",
    "    Parameters:\n",
    "    ------------\n",
    "    y   - 1d-array with shape (n,)\n",
    "    cov - 2d-array with shape (n,n)\n",
    "    '''\n",
    "    # Use Cholesky decomposition for efficient lin alg algo\n",
    "    L, info = lapack.dpotrf(cov, clean=True)\n",
    "\n",
    "    if (info < 0):\n",
    "        raise print('Error occured in computation of Cholesky decomposition')\n",
    "\n",
    "    # Solve equation L*b=y\n",
    "    b, info = lapack.dpotrs(L, np.array(y))\n",
    "\n",
    "    if (info != 0):\n",
    "        raise print('Error in inverting matrix equation')\n",
    "\n",
    "    if np.all(L.diagonal() > 0):\n",
    "        return -0.5 * np.dot(y, b) - np.log(L.diagonal()).sum()\n",
    "    else:\n",
    "        raise print('Diagonal has negative entry')\n",
    "\n",
    "def log_posterior(model_parameters, design_range, true_observables, true_errors, hydro_name, GP_emulator):\n",
    "    '''\n",
    "    Parameters:\n",
    "    ------------\n",
    "    model_parameters    - 1d-array like (n,)  \n",
    "    design_range        - 2d-array like (n,2)  \n",
    "    hydro_name          - string containing hydro theory: 'ce', 'dnmr', 'vah', 'mvah'  \n",
    "    GP_emulatr          - dictionary(hydro_name: emulator_list),  \n",
    "                            emulator_list[0] - energery density  \n",
    "                            emulator_list[1] - shear stress  \n",
    "                            emulator_list[2] - bulk stress  \n",
    "    '''\n",
    "    running_log_likelihood = 0\n",
    "    for k in range(true_observables.shape[0]):\n",
    "        emulation_values, emulation_variance = predict_observable(model_parameters, hydro_name, k, GP_emulator) \n",
    "\n",
    "        y = np.array(emulation_values).flatten() - np.array(true_observables[k]).flatten()\n",
    "        cov = emulation_variance + np.diag(true_errors) ** 2\n",
    "\n",
    "        running_log_likelihood += log_likelihood(y.flatten(), cov)\n",
    "\n",
    "    return log_prior(model_parameters, design_range) + running_log_likelihood\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nparams = len(GP_parameter_names)\n",
    "nwalkers = 20 * nparams\n",
    "nburn = 500\n",
    "nsteps = 2000\n",
    "\n",
    "# TO DO: print MCMC to pickle file\n",
    "names = ['ce', 'dnmr', 'vah', 'mvah']\n",
    "MCMC_samplers = {}\n",
    "for i, name in enumerate(names):\n",
    "    print(f\"Computing for hydro theory: {name}\")\n",
    "    starting_guesses = GP_parameter_ranges[:,0] + np.random.rand(nwalkers, nparams) * GP_design_range\n",
    "    print(\"MCMC sampling using emcee (affine-invariant ensamble sampler) with {0} walkers\".format(nwalkers))\n",
    "    with Pool() as pool:\n",
    "        sampler = emcee.EnsembleSampler(nwalkers, nparams, log_posterior, args=(GP_parameter_ranges, exact_pseudo[:, 1:4], exact_error, name, emulators))\n",
    "        print('burn in sampling started')    \n",
    "        pos = sampler.run_mcmc(starting_guesses, nburn, progress=True, store=True)\n",
    "        print(\"Mean acceptance fraction: {0:.3f} (in total {1} steps)\".format(\n",
    "                        np.mean(sampler.acceptance_fraction), nwalkers*nburn))\n",
    "        print('Burn in completed.')\n",
    "        print(\"Now running the samples\")\n",
    "        sampler.run_mcmc(initial_state=None, nsteps=nsteps, progress=True, tune=False)  \n",
    "        print(\"Mean acceptance fraction: {0:.3f} (in total {1} steps)\".format(\n",
    "                        np.mean(sampler.acceptance_fraction), nwalkers*nsteps))\n",
    "    sampler_df = pd.DataFrame(sampler.get_chain(flat=True, discard=nburn), columns=GP_parameter_names_math)\n",
    "    MCMC_samplers[name] = sampler_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My own sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(emulators['ce']).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 1000\n",
    "sampler_params = np.linspace(GP_parameter_ranges[0,0], GP_parameter_ranges[0,1], steps)\n",
    "posteriors = {}\n",
    "for i, name in enumerate(names):\n",
    "    final_arr = np.zeros(steps)\n",
    "    for j in range(steps):\n",
    "        final_arr[j] = log_posterior([sampler_params[j]], GP_parameter_ranges, exact_pseudo[:, 1:4], exact_error, name, emulators)\n",
    "    posteriors[name] = final_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(5, 2, figsize=(20, 40))\n",
    "alpha_error = [0.5, 0.25, 0.125, 0.0625, 0.0]\n",
    "for k, err in enumerate(alpha_error):\n",
    "    pseudo_err = err * exact_out[:, 1:4]\n",
    "    pseudo_dat = np.array([[np.random.normal(loc=obv, scale=0.05 * obv) for obv in exact] for exact in exact_out])\n",
    "    for i, tau in enumerate(simulation_taus):\n",
    "        exact_pseudo[i, 0] = tau\n",
    "    steps = 1000\n",
    "    sampler_params = np.linspace(GP_parameter_ranges[0,0], GP_parameter_ranges[0,1], steps)\n",
    "    posteriors = {}\n",
    "    for i, name in enumerate(names):\n",
    "        final_arr = np.zeros(steps)\n",
    "        for j in range(steps):\n",
    "            final_arr[j] = log_posterior([sampler_params[j]], GP_parameter_ranges, pseudo_dat[:, 1:4], pseudo_err, name, emulators)\n",
    "        posteriors[name] = final_arr\n",
    "\n",
    "    for i, name in enumerate(names):\n",
    "        if False:\n",
    "            ax[k, 0].plot(sampler_params, np.exp(posteriors[name] - np.max(posteriors[name])), label=name)\n",
    "        else:\n",
    "            ax[k, 0].plot(sampler_params, np.exp(np.array(posteriors[name], dtype=np.float128)) / np.sum(np.exp(np.array(posteriors[name], dtype=np.float128))), label=name)\n",
    "        ax[k, 1].plot(sampler_params, posteriors[name] - np.max(posteriors[name]))\n",
    "    ax[k, 0].legend(loc='upper right', fontsize=20)\n",
    "    costumize_axis(ax[k, 0], r'$\\mathcal C$', 'Posterior')\n",
    "    costumize_axis(ax[k, 1], r'$\\mathcal C$', 'Log-posterior')\n",
    "fig.tight_layout()\n",
    "fig.savefig('plots/1param-posterior-compare-errors1.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in names:\n",
    "    print(posteriors[name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation: Autocorrelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autocorrelation(chain, max_lag=100):\n",
    "    dim = len(chain)\n",
    "    acorr = np.empty(max_lag+1)\n",
    "    if max_lag > dim / 5:\n",
    "        print('max_lag is more than a 5th the chain length')\n",
    "    \n",
    "    chain1d = chain - np.average(chain)\n",
    "    for lag in range(max_lag + 1):\n",
    "        unshifted = None\n",
    "        shifted = chain1d[lag:]\n",
    "        if lag == 0:\n",
    "            unshifted = chain1d\n",
    "        else:\n",
    "            unshifted = chain1d[:-lag]\n",
    "        normalization = np.sqrt(np.dot(unshifted, unshifted))\n",
    "        normalization *= np.sqrt(np.dot(shifted, shifted))\n",
    "        acorr[lag] = np.dot(shifted, unshifted) / normalization\n",
    "    return acorr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 4, figsize=(40, 10))\n",
    "fig.patch.set_facecolor('white')\n",
    "cmap=get_cmap(10, 'tab10')\n",
    "for irow, name in enumerate(names):\n",
    "    for jcol, param in enumerate(GP_parameter_names_math):\n",
    "        ax[irow].plot(np.arange(MCMC_samplers[name].shape[0]), MCMC_samplers[name].iloc[:,jcol], color=cmap(irow), label=name)\n",
    "        costumize_axis(ax[irow], 'step', r'$\\mathcal C$')\n",
    "        ax[irow].legend(loc='upper right', fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 4, figsize=(40, 10))\n",
    "fig.patch.set_facecolor('white')\n",
    "cmap=get_cmap(10, 'tab10')\n",
    "for irow, name in enumerate(names):\n",
    "    for jcol, param in enumerate(GP_parameter_names_math):\n",
    "        acorr = autocorrelation(MCMC_samplers[name].iloc[:,jcol].to_numpy(), 1000)\n",
    "        ax[irow].plot(acorr, color=cmap(irow), label=name)\n",
    "        costumize_axis(ax[irow], 'lag', 'autocorrelation(' + param + ')')\n",
    "        ax[irow].legend(loc='upper right', fontsize=20)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['ce', 'dnmr', 'vah', 'mvah']\n",
    "for i, name in enumerate(names):\n",
    "    g = sns.PairGrid(MCMC_samplers[name].iloc[:,:], corner=True, diag_sharey=False)\n",
    "    g.map_lower(sns.histplot, bins=100, color=sns.color_palette()[9])\n",
    "    g.map_diag(sns.kdeplot, linewidth=2, shade=True, color=sns.color_palette()[-1])\n",
    "    # for n in range(nparams):\n",
    "    #     ax=g.axes[n][n]\n",
    "    #     ax.axvline(x=true_parameters[n], ls='-', c=sns.color_palette()[3], label='Truth')\n",
    "    #     ax.text(0,0.8,s= f'{true_parameters[n]:.2f}', transform=ax.transAxes, color=sns.color_palette()[1], fontsize=12)\n",
    "    # g.axes.legend(loc='best', fontsize=10)\n",
    "    # g.axes.text(0.5 ,0.8, s=name, color=sns.color_palette()[1], fontsize=12)\n",
    "g.fig.savefig('plots/MCMC-posteriors_n=1.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 4, figsize=(40,10))\n",
    "fig.patch.set_facecolor('white')\n",
    "\n",
    "cmap=get_cmap(10, 'tab10')\n",
    "for i, name in enumerate(names):\n",
    "    ax[i].hist(MCMC_samplers[name].iloc[:,0], bins=50, lw=2, color=cmap(i), alpha=.5, label=name)\n",
    "    ax[i].axvline(x=5/(4*np.pi), color='black')\n",
    "    ax[i].set_xlim(GP_parameter_ranges[0,0], GP_parameter_ranges[0,1])\n",
    "    ax[i].legend(loc='upper right', fontsize=25)\n",
    "fig.tight_layout()\n",
    "# fig.savefig('plots/MCMC-posteriors_n=1.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GP_parameter_ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(my_hist[0].shape)\n",
    "print(my_hist[1].shape)\n",
    "my_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Math\n",
    "parameter_names = [r'\\eta/\\mathcal{S}', r'\\tau_f']\n",
    "for name in names:\n",
    "    print(f'Prediction of {name}:')\n",
    "    for i in range(num_parameters_in_GP):\n",
    "        mcmc = np.percentile(MCMC_samplers[name].to_numpy()[:, i], [16, 50, 84])\n",
    "        q = np.diff(mcmc)\n",
    "        output = '{{{1}}}= {2:.3f}^{{+{3:.3f}}}_{{-{4:.3f}}}'.format(name, parameter_names[i], mcmc[1], q[1], q[0])\n",
    "        display(Math(output))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b9dcb5d757abc08f80c32e5cf6b134b847ce902aef6cf0ef93f55126bf17cd8b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('nucl-research': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
